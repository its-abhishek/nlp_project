{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.svm import SVC\n",
    "from transformers import BertTokenizer, BertModel\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load pre-trained BERT model and tokenizer\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "bert_model = BertModel.from_pretrained('bert-base-uncased')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize_text(text):\n",
    "    tokenized_text = tokenizer.encode_plus(\n",
    "        text,\n",
    "        add_special_tokens=True,\n",
    "        max_length=128,\n",
    "        pad_to_max_length=True,\n",
    "        truncation=True,  # Explicitly activate truncation\n",
    "        return_attention_mask=True,\n",
    "        return_tensors='pt'\n",
    "    )\n",
    "    return tokenized_text['input_ids'], tokenized_text['attention_mask']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a Support Vector Machine (SVM) classifier\n",
    "svm_classifier = SVC(kernel='linear')\n",
    "\n",
    "# Define a neural network for sentiment analysis\n",
    "class SentimentAnalysisNN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(SentimentAnalysisNN, self).__init__()\n",
    "        self.embedding = bert_model\n",
    "        self.fc = nn.Linear(768, 1)  # Output 1 because it's binary sentiment classification\n",
    "\n",
    "    def forward(self, input_ids, attention_mask):\n",
    "        _, pooled_output = self.embedding(input_ids=input_ids, attention_mask=attention_mask)\n",
    "        output = self.fc(pooled_output)\n",
    "        return torch.sigmoid(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a neural network for contextual analysis\n",
    "class ContextualAnalysisNN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(ContextualAnalysisNN, self).__init__()\n",
    "        self.embedding = bert_model\n",
    "        self.fc = nn.Linear(768, 1)  # Output 1 because it's binary classification\n",
    "\n",
    "    def forward(self, input_ids, attention_mask):\n",
    "        _, pooled_output = self.embedding(input_ids=input_ids, attention_mask=attention_mask)\n",
    "        output = self.fc(pooled_output)\n",
    "        return torch.sigmoid(output)\n",
    "    \n",
    "    # Initialize sentiment analysis neural network\n",
    "sentiment_model = SentimentAnalysisNN()\n",
    "\n",
    "# Initialize contextual analysis neural network\n",
    "contextual_model = ContextualAnalysisNN()\n",
    "\n",
    "# Define loss function and optimizer for sentiment analysis\n",
    "sentiment_criterion = nn.BCELoss()\n",
    "sentiment_optimizer = optim.Adam(sentiment_model.parameters(), lr=0.001)\n",
    "\n",
    "# Define loss function and optimizer for contextual analysis\n",
    "contextual_criterion = nn.BCELoss()\n",
    "contextual_optimizer = optim.Adam(contextual_model.parameters(), lr=0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ContextualAnalysisNN(\n",
       "  (embedding): BertModel(\n",
       "    (embeddings): BertEmbeddings(\n",
       "      (word_embeddings): Embedding(30522, 768, padding_idx=0)\n",
       "      (position_embeddings): Embedding(512, 768)\n",
       "      (token_type_embeddings): Embedding(2, 768)\n",
       "      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (encoder): BertEncoder(\n",
       "      (layer): ModuleList(\n",
       "        (0-11): 12 x BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (pooler): BertPooler(\n",
       "      (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "      (activation): Tanh()\n",
       "    )\n",
       "  )\n",
       "  (fc): Linear(in_features=768, out_features=1, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Check if GPU is available\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# Move models and data to the available device\n",
    "sentiment_model.to(device)\n",
    "contextual_model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training loop for sentiment analysis\n",
    "def train_sentiment(train_loader, num_epochs=5):\n",
    "    for _ in range(num_epochs):\n",
    "        for inputs, labels in train_loader:\n",
    "            inputs = inputs.to(device)\n",
    "            labels = labels.to(device)\n",
    "\n",
    "            sentiment_optimizer.zero_grad()\n",
    "\n",
    "            outputs = sentiment_model(*inputs)\n",
    "            loss = sentiment_criterion(outputs, labels.unsqueeze(1).float())\n",
    "            loss.backward()\n",
    "            sentiment_optimizer.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training loop for contextual analysis\n",
    "def train_contextual(train_loader, num_epochs=5):\n",
    "    for _ in range(num_epochs):  # Use _ instead of epoch as it's not being used\n",
    "        for inputs, labels in train_loader:\n",
    "            inputs = inputs.to(device)\n",
    "            labels = labels.to(device)\n",
    "\n",
    "            contextual_optimizer.zero_grad()\n",
    "\n",
    "            outputs = contextual_model(inputs)\n",
    "            loss = contextual_criterion(outputs, labels.unsqueeze(1).float())\n",
    "            loss.backward()\n",
    "            contextual_optimizer.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to predict sentiment\n",
    "def predict_sentiment(text):\n",
    "    input_ids, attention_mask = tokenize_text(text)\n",
    "    with torch.no_grad():\n",
    "        output = sentiment_model(input_ids.unsqueeze(0), attention_mask.unsqueeze(0))\n",
    "    return output.item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to predict contextual analysis\n",
    "def predict_contextual(text):\n",
    "    input_ids, attention_mask = tokenize_text(text)\n",
    "    with torch.no_grad():\n",
    "        output = contextual_model(input_ids.unsqueeze(0), attention_mask.unsqueeze(0))\n",
    "    return output.item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "too many values to unpack (expected 2)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[87], line 3\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m# Example usage\u001b[39;00m\n\u001b[0;32m      2\u001b[0m text \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mI did not steal the money.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m----> 3\u001b[0m sentiment_score \u001b[38;5;241m=\u001b[39m \u001b[43mpredict_sentiment\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtext\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m      4\u001b[0m contextual_score \u001b[38;5;241m=\u001b[39m predict_contextual(text)\n\u001b[0;32m      5\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mSentiment score:\u001b[39m\u001b[38;5;124m\"\u001b[39m, sentiment_score)\n",
      "Cell \u001b[1;32mIn[85], line 5\u001b[0m, in \u001b[0;36mpredict_sentiment\u001b[1;34m(text)\u001b[0m\n\u001b[0;32m      3\u001b[0m input_ids, attention_mask \u001b[38;5;241m=\u001b[39m tokenize_text(text)\n\u001b[0;32m      4\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mno_grad():\n\u001b[1;32m----> 5\u001b[0m     output \u001b[38;5;241m=\u001b[39m \u001b[43msentiment_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43minput_ids\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43munsqueeze\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43munsqueeze\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m      6\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m output\u001b[38;5;241m.\u001b[39mitem()\n",
      "File \u001b[1;32mc:\\Users\\abhih\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1496\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1497\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1498\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1499\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1500\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1501\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1502\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "Cell \u001b[1;32mIn[80], line 12\u001b[0m, in \u001b[0;36mSentimentAnalysisNN.forward\u001b[1;34m(self, input_ids, attention_mask)\u001b[0m\n\u001b[0;32m     11\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, input_ids, attention_mask):\n\u001b[1;32m---> 12\u001b[0m     _, pooled_output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43membedding\u001b[49m\u001b[43m(\u001b[49m\u001b[43minput_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minput_ids\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mattention_mask\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     13\u001b[0m     output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfc(pooled_output)\n\u001b[0;32m     14\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m torch\u001b[38;5;241m.\u001b[39msigmoid(output)\n",
      "File \u001b[1;32mc:\\Users\\abhih\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1496\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1497\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1498\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1499\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1500\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1501\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1502\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[1;32mc:\\Users\\abhih\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\transformers\\models\\bert\\modeling_bert.py:967\u001b[0m, in \u001b[0;36mBertModel.forward\u001b[1;34m(self, input_ids, attention_mask, token_type_ids, position_ids, head_mask, inputs_embeds, encoder_hidden_states, encoder_attention_mask, past_key_values, use_cache, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[0;32m    964\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    965\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mYou have to specify either input_ids or inputs_embeds\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m--> 967\u001b[0m batch_size, seq_length \u001b[38;5;241m=\u001b[39m input_shape\n\u001b[0;32m    968\u001b[0m device \u001b[38;5;241m=\u001b[39m input_ids\u001b[38;5;241m.\u001b[39mdevice \u001b[38;5;28;01mif\u001b[39;00m input_ids \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m inputs_embeds\u001b[38;5;241m.\u001b[39mdevice\n\u001b[0;32m    970\u001b[0m \u001b[38;5;66;03m# past_key_values_length\u001b[39;00m\n",
      "\u001b[1;31mValueError\u001b[0m: too many values to unpack (expected 2)"
     ]
    }
   ],
   "source": [
    "# Example usage\n",
    "text = \"I did not steal the money.\"\n",
    "sentiment_score = predict_sentiment(text)\n",
    "contextual_score = predict_contextual(text)\n",
    "print(\"Sentiment score:\", sentiment_score)\n",
    "print(\"Contextual score:\", contextual_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cosine Similarity Score: 0.77243125\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import BertTokenizer, BertModel\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "# Load pre-trained BERT model and tokenizer\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "model = BertModel.from_pretrained('bert-base-uncased')\n",
    "\n",
    "# Input texts\n",
    "input_text_1 = \"Young women, 'account for a shocking 74% of all new HIV infections among adolescents in Africa.'\"\n",
    "input_text_2 = \"HIV infections are disproportionately affecting young women in Africa.\"\n",
    "\n",
    "# Tokenize input texts\n",
    "input_ids_1 = tokenizer.encode(input_text_1, add_special_tokens=True, return_tensors=\"pt\")\n",
    "input_ids_2 = tokenizer.encode(input_text_2, add_special_tokens=True, return_tensors=\"pt\")\n",
    "\n",
    "# Perform forward passes\n",
    "with torch.no_grad():\n",
    "    outputs_1 = model(input_ids_1)\n",
    "    outputs_2 = model(input_ids_2)\n",
    "\n",
    "# Get the CLS embeddings\n",
    "cls_embedding_1 = outputs_1.last_hidden_state[:, 0, :]  # CLS embedding for input_text_1\n",
    "cls_embedding_2 = outputs_2.last_hidden_state[:, 0, :]  # CLS embedding for input_text_2\n",
    "\n",
    "# Convert CLS embeddings to numpy arrays\n",
    "cls_embedding_np_1 = cls_embedding_1.numpy()\n",
    "cls_embedding_np_2 = cls_embedding_2.numpy()\n",
    "\n",
    "# Calculate cosine similarity\n",
    "similarity_score = cosine_similarity(cls_embedding_np_1, cls_embedding_np_2)\n",
    "\n",
    "print(\"Cosine Similarity Score:\", similarity_score[0][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cosine Similarity Score: 0.77243125\n",
      "Euclidean Distance: 10.458046\n",
      "Jaccard Similarity Score: 0.567287784679089\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import BertTokenizer, BertModel\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from sklearn.metrics.pairwise import euclidean_distances\n",
    "import numpy as np\n",
    "\n",
    "# Load pre-trained BERT model and tokenizer\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "model = BertModel.from_pretrained('bert-base-uncased')\n",
    "\n",
    "# Input texts\n",
    "input_text_1 = \"Young women, 'account for a shocking 74% of all new HIV infections among adolescents in Africa.'\"\n",
    "input_text_2 = \"HIV infections are disproportionately affecting young women in Africa.\"\n",
    "\n",
    "# Tokenize input texts\n",
    "input_ids_1 = tokenizer.encode(input_text_1, add_special_tokens=True, return_tensors=\"pt\")\n",
    "input_ids_2 = tokenizer.encode(input_text_2, add_special_tokens=True, return_tensors=\"pt\")\n",
    "\n",
    "# Perform forward passes\n",
    "with torch.no_grad():\n",
    "    outputs_1 = model(input_ids_1)\n",
    "    outputs_2 = model(input_ids_2)\n",
    "\n",
    "# Get the CLS embeddings\n",
    "cls_embedding_1 = outputs_1.last_hidden_state[:, 0, :].detach().numpy()  # CLS embedding for input_text_1\n",
    "cls_embedding_2 = outputs_2.last_hidden_state[:, 0, :].detach().numpy()  # CLS embedding for input_text_2\n",
    "\n",
    "# Calculate cosine similarity\n",
    "cosine_similarity_score = cosine_similarity(cls_embedding_1, cls_embedding_2)\n",
    "\n",
    "# Calculate Euclidean distance\n",
    "euclidean_distance = euclidean_distances(cls_embedding_1, cls_embedding_2)\n",
    "\n",
    "# Convert embeddings to sets for Jaccard similarity\n",
    "set_1 = set(np.where(cls_embedding_1 > 0)[1])\n",
    "set_2 = set(np.where(cls_embedding_2 > 0)[1])\n",
    "\n",
    "# Calculate Jaccard similarity manually\n",
    "intersection = len(set_1.intersection(set_2))\n",
    "union = len(set_1.union(set_2))\n",
    "jaccard_similarity = intersection / union\n",
    "\n",
    "print(\"Cosine Similarity Score:\", cosine_similarity_score[0][0])\n",
    "print(\"Euclidean Distance:\", euclidean_distance[0][0])\n",
    "print(\"Jaccard Similarity Score:\", jaccard_similarity)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Word Scores:\n",
      "[CLS]: 0.37001008\n",
      "young: 0.5208982\n",
      "women: 0.54647523\n",
      ",: 0.48232278\n",
      "': 0.45623493\n",
      "account: 0.67192876\n",
      "for: 0.7431141\n",
      "a: 0.7444166\n",
      "shocking: 0.61947006\n",
      "74: 0.41476735\n",
      "%: 0.63859886\n",
      "of: 0.7561859\n",
      "all: 0.7466289\n",
      "new: 0.6660677\n",
      "hiv: 0.5798317\n",
      "infections: 0.6483834\n",
      "among: 0.6813404\n",
      "adolescents: 0.58263814\n",
      "in: 0.6916919\n",
      "africa: 0.5170099\n",
      ".: 0.32017565\n",
      "[SEP]: 0.31911638\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import BertTokenizer, BertModel\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "import numpy as np\n",
    "\n",
    "# Load pre-trained BERT model and tokenizer\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "model = BertModel.from_pretrained('bert-base-uncased')\n",
    "\n",
    "# Input text\n",
    "input_text = \"Young women, 'account for a shocking 74% of all new HIV infections among adolescents in Africa.'\"\n",
    "\n",
    "# Tokenize input text\n",
    "input_ids = tokenizer.encode(input_text, add_special_tokens=True, return_tensors=\"pt\")\n",
    "\n",
    "# Perform forward pass\n",
    "with torch.no_grad():\n",
    "    outputs = model(input_ids)\n",
    "\n",
    "# Get the contextual embeddings for each token\n",
    "contextual_embeddings = outputs.last_hidden_state[0]  # Batch dimension is 1\n",
    "\n",
    "# Convert contextual embeddings to numpy array\n",
    "contextual_embeddings_np = contextual_embeddings.numpy()\n",
    "\n",
    "# Get the vocabulary from the tokenizer\n",
    "vocab = tokenizer.get_vocab()\n",
    "\n",
    "# Map indices to tokens\n",
    "indices_to_tokens = {idx: token for token, idx in vocab.items()}\n",
    "\n",
    "# Find the top similar words for each word and calculate scores\n",
    "word_scores = {}\n",
    "for i in range(len(input_ids[0])):\n",
    "    word = indices_to_tokens[input_ids[0][i].item()]\n",
    "    word_embedding = contextual_embeddings_np[i].reshape(1, -1)\n",
    "    similarity_scores = cosine_similarity(word_embedding, contextual_embeddings_np)[0]\n",
    "    most_similar_indices = np.argsort(similarity_scores)[::-1][1:6]  # Exclude the word itself\n",
    "    most_similar_words = [indices_to_tokens[idx] for idx in most_similar_indices]\n",
    "    average_similarity = np.mean(similarity_scores[most_similar_indices])\n",
    "    word_scores[word] = average_similarity\n",
    "\n",
    "# Print scores\n",
    "print(\"Word Scores:\")\n",
    "for word, score in word_scores.items():\n",
    "    print(word + \":\", score)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
